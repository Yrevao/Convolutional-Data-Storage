Introduction: 
	Machine learning has been on the rise in recent years due to the popularity of neural networks by allowing previously very difficult or impossible to automate problems to be automated relatively easily. An example of this is image recognition, before one would have to write a complex mathematical algorithm to analyze groups of pixels and determine if the image is of a cat or a dog, now instead of having to write the algorithm by hand a neural network can be trained to differentiate between images of cats and dogs with good enough accuracy. As with most emerging technologies they take on many more roles as they become established, neural networks are no different as they can be seen everywhere nowadays from neural network driven chat bots being used as programming assistants to protein folding simulations being massively sped up using neural networks. Theoretically neural networks should be capable of solving any problem since they function similarly to the human brain, however that does not mean there aren't limitations to their problem solving like the computing power needed to drive the neural network models and time needed to train the models. These limitations make up most of the work that goes into applying a neural network to a given problem in a workable way.

Body: 
	One way that most people have interacted with machine learning technology is through ChatGPT which is a chatbot that makes use of OpenAI’s GPT-3.5 model to provide an acceptable response to a wide range of prompts. Programming assistance has become one of the major fields that ChatGPT has proven to be proficient in. When compared to existing programming assistance tools such as Stack Overflow programmers using ChatGPT have been shown to produce better quality code in a shorter time frame when working with problems involving algorithms and libraries (Liu et al., 2023); this shows that for most types of problems in this category information generated by a neural network is easier to access than information posted to an online forum. However, over reliance on ChatGPT for solving problems can lead to less creative solutions and an overall reduction in quality of work due to errors made by ChatGPT in relation to how it interprets a given prompt (Liu et al., 2023), as well as outright hallucinations of information communicated as fact (Brodeur, 2023). Also the information gathered by ChatGPT does not go further than September of 2021 (Brodeur, 2023) which makes a difficult case for programmers and other professionals working on the cutting edge or who need access to very up to date facts. ChatGPT and other models are also owned by a third party making their use a confidentiality issue since any information given to ChatGPT is also being handed over to OpenAI for free (Brodeur, 2023). These limitations make the use of ChatGPT more difficult or legally infeasible for tax and legal professionals due to the limitations of Circular 230 which place significant requirements on written advice requiring that such advice is based on fact which ChatGPT struggles with due to the aforementioned reasons (Brodeur, 2023). This makes ChatGPT more of a casual tool rather than a trusted assistant.
	Machine learning can be used to speed up algorithmic processes, one such case is for protein structure prediction where AlphaFold provided a significantly improved method both in terms of accuracy and computation speed when compared to previous physics based techniques (Gordon et al., 2023). In the 2023 paper about using AlphaFold 2 for predicting the structure of ribosomally synthesized and post-translationally modified peptides, AlphaFold was able to predict the structure of the proteins with TMscores ranging from 0.8794 to 0.9979 considering that a TMscore of 1 indicates a perfect match to experimental results these scores are considerably good (Gordon et al., 2023). A common problem with neural networks is fragility which is when the network passes the tests used in training, but fails to recognize data outside of the training set even if that data only differs slightly from the training material (Johnson & Bullock, 2023). This effect also occurs in animal brains since they function on similar principles to neural networks, a good example of this is the concentric circles illusion where changing the orientation of the black bars around the circles can make the circles appear as spirals instead of circles (Johnson & Bullock, 2023). Animal brains combat this effect somewhat by instead of having one big neural network they have many smaller neural networks with varying structures integrating information between each other in a competitive manner to reach a final result which makes errors less distracting to the end user (Johnson & Bullock, 2023). The environmental impact of developing and deploying neural networks should also be taken into consideration. Unlike the development of conventional algorithms that use the time of several experienced programmers, neural networks take a large computing cluster a somewhat long period of time ranging from a week or so to several years to get the network trained to a state that is considered acceptable to deploy. The process of running these large computing clusters requires a lot of wattage that has to be generated via potentially non-renewable resources; this makes estimating the carbon footprint of neural networks an emerging and important issue as neural networks become more widespread causing more training to be performed (Heguerte et al., 2023). The deployment cost of the network should also be considered, because once the network is trained data still has to be propagated through the network which can consist of many thousands of nodes consuming power in the process (Heguerte et al., 2023).
	
Conclusion:
	As machine learning becomes a more established technology it is necessary to understand how to best apply the technology to various problems in an efficient and error resistant manner to best respect its current shortcomings. When used properly neural networks are powerful tools that allow previously thought infeasible algorithmic problems to be solved with efficiency and great optimization to be made to others. However when not applied to the proper problems or when poorly implemented neural networks pose a threat to data integrity and the efficient use of computing resources due to the massive amount of energy required to train and deploy a neural network. Hallucination and fragility are problems that plague machine learning due to the low fidelity of the neural networks currently in use today and the lack of understanding towards how neural networks actually organize the associations of data trained into them. As computing technology advances more competent and complex neural networks will come into operation hopefully fixing some of their current shortcomings.

References
Brodeur, Garrett L., et al. “ChatGPT for Legal and Tax Professionals.” CPA Journal, vol. 93, no. 7/8, July 2023, pp. 68–71. EBSCOhost, https://search.ebscohost.com/login.aspx?direct=true&AuthType=ip,shib&db=bth&AN=170389250&site=eds-live&scope=site&custid=ns235470.
Gordon, C., Hendrix, E., He, Y., & Walker, M. C. (2023). AlphaFold Accurately Predicts the Structure of Ribosomally Synthesized and Post-Translationally Modified Peptide Biosynthetic Enzymes. Biomolecules, 13(8), 1243–1243. https://doi.org/10.3390/biom13081243.
Heguerte, L. B., Bugeau, A., & Lannelongue, L. (2023). How to estimate carbon footprint when training deep learning models? A guide and review. Environmental Research Communications. https://doi.org/10.1088/2515-7620/acf81b.
Johnson, J., & Bullock, D. (2023). Fragility in AIs Using Artificial Neural Networks. Communications of the ACM, 66(7), 28–31. https://doi.org/10.1145/3571280.
Liu, J., Tang, X., Li, L., Chen, P., & Liu, Y. (2023, August 26). Which is a better programming assistant? A comparative study between chatgpt and stack overflow. ArXiv.org. https://doi.org/10.48550/arXiv.2308.13851.
